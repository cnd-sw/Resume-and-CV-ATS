{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57ed8e2a-8189-407b-bbc1-b8a57ceb74f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyPDF2 nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f11b95f-a938-4d6d-ad1b-090670d29dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/chandan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3affacff-9244-4496-8c2c-eb684e57db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3fabb2f-dd2e-409e-93ba-4218e8e322e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38af4b02-8b00-434b-a958-2f8742658f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, num_sentences=10):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Calculate word frequency distribution\n",
    "    freq_dist = FreqDist(words)\n",
    "\n",
    "    # Rank sentences based on word frequency\n",
    "    sentences = text.split('.')\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word in freq_dist:\n",
    "                if len(sentence.split(' ')) < 30:\n",
    "                    if sentence not in sentence_scores:\n",
    "                        sentence_scores[sentence] = freq_dist[word]\n",
    "                    else:\n",
    "                        sentence_scores[sentence] += freq_dist[word]\n",
    "\n",
    "    # Select top sentences for summary\n",
    "    summary_sentences = nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2b589b1-f769-46fa-9edf-d1197055772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    pdf_path = input(\"Enter the path to the PDF file: \")\n",
    "    resume_text = extract_text_from_pdf(pdf_path)\n",
    "    summary = summarize_text(resume_text)\n",
    "    print(\"\\nSummary of the resume:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dee43e5e-9df4-425e-84c9-a953d0f53c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to the PDF file:  /Users/chandan/Downloads/Chandan_Kumar_Sangewar_Resume.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of the resume:\n",
      " \n",
      "Technologies/Libraries : Numpy, Pandas, PySpark, Matplotlib, Tensorflow, Scikit-learn, Keras, NLTK \n",
      "An Exploration of Session Based Recommendation System Nov 2023 – Present\n",
      "Python, ML, DL, NLP, LSTM, Graph Based ML\n",
      "•Working of session based recommendation system in e-commerce \n",
      "Developer/Visualisation Tools : VS Code, Google Colab, R Studio, Tableau, SPSS, Stata, Databricks 42 (4 Scale)\n",
      "Skills\n",
      "Course Work : Machine Learning(ML), Natural Language Processing(NLP), Deep Learning(DL),Data Analysis S051 — Python, Big Data, ML, DL, NLP\n",
      "•A recommendation system for movies and web series employed collaborative filtering on the MovieLens-25M Data \n",
      "•Was able to score essays based on cohesion, syntax, vocabulary, phraseology, grammar and conventions making the\n",
      "evaluation job easier \n",
      "Languages : R, Python, NoSQL, MongoDB com/cnd-sw\n",
      "Education\n",
      "Vellore Institute of Technology, Chennai, India Jul 2020 – Jul 2025\n",
      "Integrated Masters in Computer Science and Engineering with Specialisation in Business Analytics CGPA: 3 \n",
      "•Developed a functioning model using the Random Forest algorithm and General Neural Network , The model\n",
      "attained an amazing accuracy of 98  LSTM model shows promising results, but\n",
      "Graph-Based ML model approach wins in real-world tests\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e623bba8-332b-4a54-961a-1d6930cf5d16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
